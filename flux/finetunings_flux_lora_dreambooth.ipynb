{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetunings](https://github.com/jhj0517/finetunings) repository!**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #1. Install Dependencies\n",
        "\n",
        "!git clone https://github.com/ostris/ai-toolkit\n",
        "!mkdir -p /content/dataset\n",
        "!cd ai-toolkit && git submodule update --init --recursive && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training image dataset.\n",
        "\n",
        "#@markdown The dataset should have following structure:\n",
        "\n",
        "#@markdown Each image file should have a corresponding text file (`.txt`) with the same name.\n",
        "#@markdown The text file typically contains prompts or metadata associated with the image.\n",
        "\n",
        "#@markdown ### Example File Structure:\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown â”œâ”€â”€ a (1).png         # Image file\n",
        "#@markdown â”œâ”€â”€ a (1).txt         # Corresponding prompt for a (1).png\n",
        "#@markdown â”œâ”€â”€ a (2).png         # Another image file\n",
        "#@markdown â”œâ”€â”€ a (2).txt         # Corresponding prompt for a (2).png\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1b7718-0378-4ba3-db04-16f3d2e34077"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown If you don't have entire base model files ([black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)) in the drive you need to sign in to Huggingface to download the model.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kqKOGVkupS_R2FQ_049xLw.png))\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "#os.environ['HF_TOKEN'] = getpass.getpass(hf_token)\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9WzQRwZij5jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c3c2f4-cdb2-4fae-da3a-111cf9391dc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF_TOKEN environment variable has been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/ai-toolkit')\n",
        "from toolkit.job import run_job\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "#@markdown ## Paths\n",
        "\n",
        "#@markdown Set your dataset path and output path for lora here.\n",
        "DATASET_DIR = \"/content/drive/MyDrive/finetunings/dataset\" # @param {type:\"string\"}\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/finetunings/trained-flux'  # @param {type:\"string\"}\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#@markdown ## Base Model\n",
        "#@markdown If you'll just use the default repo id here then you need to signin to huggingface in the previous section\n",
        "repo_id_or_path = 'black-forest-labs/FLUX.1-dev' # @param {type:\"string\"}\n",
        "quantize = False # @param {type:\"boolean\"}\n",
        "low_vram = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Process\n",
        "#@markdown (max_step_saves_to_keep = how many checkpoints to keep. )\n",
        "dtype = \"float16\" # @param {type:\"string\"}\n",
        "save_every = 250 # @param {type:\"integer\"}\n",
        "max_step_saves_to_keep = 4 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown ## Hyper Parameters\n",
        "batch_size = 1 # @param {type:\"integer\"}\n",
        "steps = 500 # @param {type:\"integer\"}\n",
        "gradient_accumulation_steps = 1 # @param {type:\"integer\"}\n",
        "train_dtype = \"bf16\" # @param {type:\"string\"}\n",
        "lr = 4e-4 # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "job_to_run = OrderedDict([\n",
        "    ('job', 'extension'),\n",
        "    ('config', OrderedDict([\n",
        "        # this name will be the folder and filename name\n",
        "        ('name', 'my_first_flux_lora_v1'),\n",
        "        ('process', [\n",
        "            OrderedDict([\n",
        "                ('type', 'sd_trainer'),\n",
        "                ('training_folder', OUTPUT_DIR),\n",
        "                ('performance_log_every', 1000),\n",
        "                ('device', 'cuda:0'),\n",
        "                ('network', OrderedDict([\n",
        "                    ('type', 'lora'),\n",
        "                    ('linear', 16),\n",
        "                    ('linear_alpha', 16)\n",
        "                ])),\n",
        "                ('save', OrderedDict([\n",
        "                    ('dtype', dtype),  # precision to save\n",
        "                    ('save_every', save_every),  # save every this many steps\n",
        "                    ('max_step_saves_to_keep', max_step_saves_to_keep)  # how many intermittent saves to keep\n",
        "                ])),\n",
        "                ('datasets', [\n",
        "                    OrderedDict([\n",
        "                        ('folder_path', DATASET_DIR),\n",
        "                        ('caption_ext', 'txt'),\n",
        "                        ('caption_dropout_rate', 0.05),  # will drop out the caption 5% of time\n",
        "                        ('shuffle_tokens', False),  # shuffle caption order, split by commas\n",
        "                        ('cache_latents_to_disk', True),  # leave this true unless you know what you're doing\n",
        "                        ('resolution', [512, 768, 1024])  # flux enjoys multiple resolutions\n",
        "                    ])\n",
        "                ]),\n",
        "                ('train', OrderedDict([\n",
        "                    ('batch_size', batch_size),\n",
        "                    ('steps', steps),  # total number of steps to train 500 - 4000 is a good range\n",
        "                    ('gradient_accumulation_steps', gradient_accumulation_steps),\n",
        "                    ('train_unet', True),\n",
        "                    ('train_text_encoder', False),  # probably won't work with flux\n",
        "                    ('content_or_style', 'balanced'),  # content, style, balanced\n",
        "                    ('gradient_checkpointing', True),  # need the on unless you have a ton of vram\n",
        "                    ('noise_scheduler', 'flowmatch'),  # for training only\n",
        "                    ('optimizer', 'adamw8bit'),\n",
        "                    ('lr', lr),\n",
        "                    # uncomment this to skip the pre training sample\n",
        "                    #('skip_first_sample', True),\n",
        "\n",
        "                    # ema will smooth out learning, but could slow it down. Recommended to leave on.\n",
        "                    ('ema_config', OrderedDict([\n",
        "                        ('use_ema', True),\n",
        "                        ('ema_decay', 0.99)\n",
        "                    ])),\n",
        "\n",
        "                    # will probably need this if gpu supports it for flux, other dtypes may not work correctly\n",
        "                    ('dtype', train_dtype)\n",
        "                ])),\n",
        "                ('model', OrderedDict([\n",
        "                    # huggingface model name or path\n",
        "                    ('name_or_path', repo_id_or_path),\n",
        "                    ('is_flux', True),\n",
        "                    ('quantize', quantize),  # run 8bit mixed precision\n",
        "                    ('low_vram', low_vram),  # uncomment this if the GPU is connected to your monitors. It will use less vram to quantize, but is slower.\n",
        "                ])),\n",
        "                ('sample', OrderedDict([\n",
        "                    ('sampler', 'flowmatch'),  # must match train.noise_scheduler\n",
        "                    ('sample_every', 250),  # sample every this many steps\n",
        "                    ('width', 1024),\n",
        "                    ('height', 1024),\n",
        "                    ('prompts', [\n",
        "                        # Change the sample prompts as you want\n",
        "                        'a futuristic cityscape at dusk, flying cars, neon lights, and a glowing sky',\n",
        "                        'a woman chef cooking gourmet food in a high-tech kitchen, surrounded by advanced gadgets',\n",
        "                        'a woman sitting by the fireplace in a cozy cabin, sipping tea, snowstorm visible through the window',\n",
        "                        'a cowboy riding a dinosaur in the wild west, holding a lasso, with a sunset in the background',\n",
        "                        'a polar bear wearing a scarf knitting in a cozy cabin, snowstorm visible through the window',\n",
        "                        'a steampunk airship floating above the clouds, gears and steam visible, with people onboard',\n",
        "                        'a futuristic racecar on a glass racetrack, glowing wheels, with a cyberpunk city in the background',\n",
        "                        'a young woman painting a giant mural on a city wall, vibrant colors, passerby watching',\n",
        "                        'a samurai standing on a mountain peak, sword drawn, cherry blossoms floating in the air',\n",
        "                        'a dragon lounging in a modern living room, reading a book by the fireplace'\n",
        "                    ]),\n",
        "                    ('neg', ''),  # not used on flux\n",
        "                    ('seed', 42),\n",
        "                    ('walk_seed', True),\n",
        "                    ('guidance_scale', 4),\n",
        "                    ('sample_steps', 20)\n",
        "                ]))\n",
        "            ])\n",
        "        ])\n",
        "    ])),\n",
        "    # you can add any additional meta info here. [name] is replaced with config name at top\n",
        "    ('meta', OrderedDict([\n",
        "        ('name', '[name]'),\n",
        "        ('version', '1.0')\n",
        "    ]))\n",
        "])\n",
        "\n",
        "run_job(job_to_run)\n"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQroYRRZzQiN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Test Your Lora\n",
        "#@markdown Once the installation is complete, you can use public URL that is displayed.\n",
        "\n",
        "# https://huggingface.co/docs/diffusers/main/api/pipelines/flux\n",
        "\n",
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "\n",
        "pipe = FluxPipeline.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.load_lora_weights(\"black-forest-labs/FLUX.1-Canny-dev-lora\")\n",
        "\n",
        "prompt = \"A cat holding a sign that says hello world\"\n",
        "out = pipe(\n",
        "    prompt=prompt,\n",
        "    guidance_scale=0.,\n",
        "    height=768,\n",
        "    width=1360,\n",
        "    num_inference_steps=4,\n",
        "    max_sequence_length=256,\n",
        ").images[0]\n",
        "out.save(\"image.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}