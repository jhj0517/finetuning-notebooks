{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.1\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown To train Wan Video lora 24GB VRAM is recommended.\n",
        "#@markdown  <br>If your dataset contains videos, then more than 24GB is recommended.\n",
        "#@markdown <br>You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/tdrussell/diffusion-pipe\n",
        "!git clone --recurse-submodules https://github.com/tdrussell/diffusion-pipe\n",
        "%cd diffusion-pipe\n",
        "\n",
        "# Cherry picked dependencies to use in Colab.\n",
        "!pip install deepspeed\n",
        "!pip install datasets\n",
        "!pip install torch-optimi\n",
        "!pip install bitsandbytes\n",
        "!pip install av\n",
        "!pip install loguru\n",
        "!pip install flash-attn\n",
        "!pip install ftfy\n",
        "!pip install dashscope\n",
        "!pip install gradio\n",
        "\n",
        "\n",
        "# Comment on the requirements above, and uncomment below if you're not using Colab.\n",
        "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install deepspeed\n",
        "# !pip install toml\n",
        "# !pip install transformers\n",
        "# !pip install diffusers>=0.32.1\n",
        "# !pip install datasets\n",
        "# !pip install pillow\n",
        "# !pip install sentencepiece\n",
        "# !pip install protobuf\n",
        "# !pip install peft\n",
        "# !pip install torch-optimi\n",
        "# !pip install tensorboard\n",
        "# !pip install tqdm\n",
        "# !pip install safetensors\n",
        "# !pip install bitsandbytes\n",
        "# !pip install imageio[ffmpeg]\n",
        "# !pip install av\n",
        "# !pip install einops\n",
        "# !pip install accelerate\n",
        "# !pip install loguru\n",
        "# !pip install flash-attn; sys_platform==linux\n",
        "# !pip install omegaconf\n",
        "# !pip install iopath\n",
        "# !pip install termcolor\n",
        "# !pip install hydra-core\n",
        "# !pip install ftfy\n",
        "# !pip install dashscope\n",
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training dataset.\n",
        "\n",
        "#@markdown When training Hunyuan Lora, the dataset could contatin both images and videos.\n",
        "\n",
        "#@markdown Each file should have a corresponding text file (`.txt`) with the same name. <br>\n",
        "#@markdown **Each video must have a specific number of frames, as much as you will define later in \"frame_buckets\".**\n",
        "\n",
        "#@markdown The text file contains prompts associated with the video or image.\n",
        "\n",
        "\n",
        "#@markdown ### Example Dataset Structure:\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown â”œâ”€â”€ a (1).mp4         # Video file\n",
        "#@markdown â”œâ”€â”€ a (1).txt         # Corresponding prompt for a (1).mp4\n",
        "#@markdown â”œâ”€â”€ a (2).mp4         # Another video file\n",
        "#@markdown â”œâ”€â”€ a (2).txt         # Corresponding prompt for a (2).mp4\n",
        "#@markdown â”œâ”€â”€ a (3).png         # Image file\n",
        "#@markdown â”œâ”€â”€ a (3).txt         # Corresponding prompt for a (3).png\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown This cell will download base models. If you don't already have the base model files in your google drive, run this.\n",
        "\n",
        "#@markdown You need Huggingface token (Read permission) to run this.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://media.githubusercontent.com/media/jhj0517/finetuning-notebooks/master/docs/screenshots/colab_secrets.png)\n",
        "\n",
        "import huggingface_hub\n",
        "import os\n",
        "\n",
        "# Set params\n",
        "BASE_MODELS_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/wan/base_models\" # @param {type:\"string\"}\n",
        "REPO_ID = \"Wan-AI/Wan2.1-T2V-1.3B\" # @param [\"Wan-AI/Wan2.1-T2V-1.3B\", \"Wan-AI/Wan2.1-T2V-14B\", \"Wan-AI/Wan2.1-I2V-14B-480P\", \"Wan-AI/Wan2.1-I2V-14B-720P\"]\n",
        "\n",
        "#@markdown Models will be downloaded from\n",
        "#@markdown - https://huggingface.co/Wan-AI\n",
        "\n",
        "repo_id, name = REPO_ID.split(\"/\")\n",
        "base_model_dir = os.path.join(BASE_MODELS_DIR_PATH, name)\n",
        "os.makedirs(base_model_dir, exist_ok=True)\n",
        "\n",
        "huggingface_hub.snapshot_download(\n",
        "    REPO_ID,\n",
        "    local_dir=base_model_dir,\n",
        "    ignore_patterns=[\"*.JPG\", \"*.png\", \"*.jpg\"],\n",
        ")"
      ],
      "metadata": {
        "id": "9WzQRwZij5jf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import sys\n",
        "import toml\n",
        "\n",
        "#@markdown If you're intended to train Lora from previous checkpoint, check this.\n",
        "RESUME_FROM_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Paths Configuration\n",
        "OUTPUT_LORA_NAME = \"My-Wan-T2V-1.3B-Lora-V1\" # @param {type:\"string\"}\n",
        "OUTPUT_LORA_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/wan/outputs\"  # @param {type:\"string\"}\n",
        "BASE_CKPT_PATH = \"/content/drive/MyDrive/finetuning-notebooks/wan/base_models/Wan2.1-T2V-1.3B\" # @param {type:\"string\"}\n",
        "DATASET_PATH = \"/content/drive/MyDrive/finetuning-notebooks/dataset/dog\" # @param {type:\"string\"}\n",
        "\n",
        "OUTPUT_LORA_DIR_PATH = os.path.join(OUTPUT_LORA_DIR_PATH, OUTPUT_LORA_NAME)\n",
        "\n",
        "#@markdown ## Dataset Configuration\n",
        "#@markdown - **`frame_buckets`** is the list of frame numbers in your dataset.\n",
        "#@markdown <br>For example, if your dataset contains 30, 60 frames of videos, then use : [30, 60]\n",
        "#@markdown <br>Don't use too long frames unless you don't have a lot of VRAM.\n",
        "#@markdown <br>If your dataset also contains images, then use : [1, 30, 60]\n",
        "#@markdown - **`resolutions`** is the list of resolutions which **`diffusion-pipe`** will resize your dataset.\n",
        "#@markdown <br>**`diffusion-pipe`** is smart to handle resizing your dataset by 1:2 or 2:1 image etc.\n",
        "#@markdown <br>If you have less than 24GB of VRAM, just set it to 512, then increase it according to your device.\n",
        "## Frame Buckets Settings\n",
        "frame_buckets = [1]  # @param {type:\"raw\"}\n",
        "# You can use 1024 if you have 24 GB > VRAM.\n",
        "resolutions = [512]  # @param {type:\"raw\"}\n",
        "## Aspect Ratio Bucketing Settings\n",
        "enable_ar_bucket = True  # @param {type:\"boolean\"}\n",
        "min_ar = 0.5  # @param {type:\"number\"}\n",
        "max_ar = 2.0  # @param {type:\"number\"}\n",
        "num_ar_buckets = 7  # @param {type:\"integer\"}\n",
        "# Reduce as necessary\n",
        "num_repeats = 5  # @param {type:\"integer\"}\n",
        "\n",
        "# Write dataset.toml\n",
        "dataset_config = {\n",
        "    \"resolutions\": resolutions,\n",
        "    \"frame_buckets\": frame_buckets,\n",
        "\n",
        "    \"enable_ar_bucket\": enable_ar_bucket,\n",
        "    \"min_ar\": min_ar,\n",
        "    \"max_ar\": max_ar,\n",
        "    \"num_ar_buckets\": num_ar_buckets,\n",
        "    \"directory\": [\n",
        "        {\n",
        "            \"path\": DATASET_PATH,\n",
        "            \"num_repeats\": num_repeats,\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "os.makedirs(OUTPUT_LORA_DIR_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "dataset_config_file_path = os.path.join(DATASET_PATH, \"dataset.toml\")\n",
        "with open(dataset_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(dataset_config, toml_file)\n",
        "print(f\"dataset.toml is saved to {dataset_config_file_path}\")\n",
        "\n",
        "#@markdown ## Base Model Configuration\n",
        "\n",
        "model_type = 'wan'\n",
        "dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "# transformer_dtype = 'float8'  # @param {type:\"string\"}\n",
        "timestep_sample_method = 'logit_normal'  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Training Settings\n",
        "epochs = 50  # @param {type:\"integer\"}\n",
        "# Batch size of a single forward/backward pass for one GPU.\n",
        "micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "# Pipeline parallelism degree. A single instance of the model is divided across this many GPUs.\n",
        "pipeline_stages = 1  # @param {type:\"integer\"}\n",
        "gradient_accumulation_steps = 4  # @param {type:\"integer\"}\n",
        "gradient_clipping = 1.0  # @param {type:\"number\"}\n",
        "# Learning rate warmup.\n",
        "warmup_steps = 50  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Eval Settings\n",
        "eval_every_n_epochs = 1  # @param {type:\"integer\"}\n",
        "eval_before_first_step = True  # @param {type:\"boolean\"}\n",
        "eval_micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "eval_gradient_accumulation_steps = 1  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Lora Settings\n",
        "adapter_type = 'lora'\n",
        "rank = 32  # @param {type:\"integer\"}\n",
        "adapter_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "\n",
        "# Optimizer settings\n",
        "optimizer_type = 'adamw_optimi'  # @param {type:\"string\"}\n",
        "lr = 2e-5  # @param {type:\"number\"}\n",
        "betas = [0.9, 0.99]  # @param {type:\"raw\"}\n",
        "weight_decay = 0.02  # @param {type:\"number\"}\n",
        "eps = 1e-8  # @param {type:\"number\"}\n",
        "\n",
        "#@markdown ## Misc Settings\n",
        "save_every_n_epochs = 10  # @param {type:\"integer\"}\n",
        "checkpoint_every_n_minutes = 30  # @param {type:\"integer\"}\n",
        "activation_checkpointing = True  # @param {type:\"boolean\"}\n",
        "partition_method = 'parameters'  # @param {type:\"string\"}\n",
        "save_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "caching_batch_size = 1  # @param {type:\"integer\"}\n",
        "steps_per_print = 1  # @param {type:\"integer\"}\n",
        "video_clip_mode = 'single_middle'  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Write config.toml\n",
        "train_config = {\n",
        "    \"output_dir\": OUTPUT_LORA_DIR_PATH,\n",
        "    \"dataset\": dataset_config_file_path,\n",
        "\n",
        "    # Training Settings\n",
        "    \"epochs\": epochs,\n",
        "    \"micro_batch_size_per_gpu\": micro_batch_size_per_gpu,\n",
        "    \"pipeline_stages\": pipeline_stages,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"gradient_clipping\": gradient_clipping,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "\n",
        "    # Eval Settings\n",
        "    \"eval_every_n_epochs\": eval_every_n_epochs,\n",
        "    \"eval_before_first_step\": eval_before_first_step,\n",
        "    \"eval_micro_batch_size_per_gpu\": eval_micro_batch_size_per_gpu,\n",
        "    \"eval_gradient_accumulation_steps\": eval_gradient_accumulation_steps,\n",
        "\n",
        "    # Misc Settings\n",
        "    \"save_every_n_epochs\": save_every_n_epochs,\n",
        "    \"checkpoint_every_n_minutes\": checkpoint_every_n_minutes,\n",
        "    \"activation_checkpointing\": activation_checkpointing,\n",
        "    \"partition_method\": partition_method,\n",
        "    \"save_dtype\": save_dtype,\n",
        "    \"caching_batch_size\": caching_batch_size,\n",
        "    \"steps_per_print\": steps_per_print,\n",
        "    \"video_clip_mode\": video_clip_mode,\n",
        "\n",
        "    \"model\": {\n",
        "        \"type\": model_type,\n",
        "        \"ckpt_path\": BASE_CKPT_PATH,\n",
        "        \"dtype\": dtype,\n",
        "        # \"transformer_dtype\": transformer_dtype,\n",
        "        \"timestep_sample_method\": timestep_sample_method,\n",
        "    },\n",
        "\n",
        "    \"adapter\": {\n",
        "        \"type\": \"lora\",\n",
        "        \"rank\": rank,\n",
        "        \"dtype\": adapter_dtype,\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": optimizer_type,\n",
        "        \"lr\": lr,\n",
        "        \"betas\": betas,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"eps\": eps,\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "train_config_file_path = os.path.join(DATASET_PATH, \"config.toml\")\n",
        "with open(train_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(train_config, toml_file)\n",
        "print(f\"config.toml is saved to {train_config_file_path}\")\n",
        "\n",
        "\n",
        "## Train\n",
        "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
        "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path} --resume_from_checkpoint\n",
        "else:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path}\n"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}