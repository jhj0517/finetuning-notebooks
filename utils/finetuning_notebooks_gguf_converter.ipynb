{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.0\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is The GGUF Model Format?\n",
        "The GGUF format is made by llama.cpp team to support various quantization.\n",
        "\n",
        "For more information about the diffusers format, you can read :\n",
        "- https://huggingface.co/docs/transformers/gguf#gguf-and-interaction-with-transformers"
      ],
      "metadata": {
        "id": "-BQeuemmkTD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "#@markdown You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2ad818-d892-430a-a1b3-8543d5d727a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 44397, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 44397 (delta 40), reused 20 (delta 20), pack-reused 44333 (from 3)\u001b[K\n",
            "Receiving objects: 100% (44397/44397), 91.25 MiB | 16.12 MiB/s, done.\n",
            "Resolving deltas: 100% (31991/31991), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/ggml-org/llama.cpp\n",
        "!git clone https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown If your model file is in the Google Drive, you can mount the Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c993fa-dbd9-4cf7-9194-52c058614880"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. Convert to GGUF\n",
        "import os\n",
        "\n",
        "#@markdown ### Path Configuration\n",
        "BASE_MODEL_PATH = \"/content/drive/MyDrive/finetuning-notebooks/flux/base_models/FLUX-dev/transformer\" #@param {type:\"string\"}\n",
        "OUTPUT_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/flux/base_models/GGUF\" #@param {type:\"string\"}\n",
        "GGUF_NAME = \"Flux.1-dev\" #@param {type:\"string\"}\n",
        "\n",
        "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "CONVERSION_SCRIPT = \"convert_hf_to_gguf.py\"\n",
        "\n",
        "def generate_importance_matrix_command(fp16_path, train_data_path, imatrix_path):\n",
        "    print(\"Importance matrix generation command requested...\")\n",
        "    command = f\"python generate_imatrix.py {fp16_path} {train_data_path} {imatrix_path}\"\n",
        "    return command\n",
        "\n",
        "def process_model_command(model_dir_path,\n",
        "                          model_name,\n",
        "                          output_dir,\n",
        "                          q_method,\n",
        "                          use_imatrix,\n",
        "                          imatrix_q_method,\n",
        "                          train_data_file):\n",
        "    commands_list = []\n",
        "    fp16_path = str(Path(output_dir)/f\"{model_name}-fp16.gguf\")\n",
        "    conversion_command = [\n",
        "        \"python\", CONVERSION_SCRIPT,\n",
        "        str(model_dir_path),\n",
        "        \"--outtype\", \"f16\",\n",
        "        \"--outfile\", fp16_path\n",
        "    ]\n",
        "    conversion_command_str = \" \".join(conversion_command)\n",
        "    commands_list.append(conversion_command_str)\n",
        "\n",
        "    imatrix_path = str(Path(output_dir)/\"imatrix.dat\")\n",
        "    if use_imatrix:\n",
        "        if train_data_file:\n",
        "            train_data_path = train_data_file.name # Assuming train_data_file is a file object\n",
        "        else:\n",
        "            train_data_path = \"llama.cpp/groups_merged.txt\" #fallback calibration dataset\n",
        "\n",
        "        imatrix_gen_command_str = generate_importance_matrix_command(fp16_path, train_data_path, imatrix_path) # Assuming this returns a command string\n",
        "        commands_list.append(imatrix_gen_command_str)\n",
        "\n",
        "\n",
        "    quantized_gguf_name = f\"{model_name.lower()}-{imatrix_q_method.lower()}-imat.gguf\" if use_imatrix else f\"{model_name.lower()}-{q_method.lower()}.gguf\"\n",
        "    quantized_gguf_path = str(Path(output_dir)/quantized_gguf_name)\n",
        "    if use_imatrix:\n",
        "        quantize_ggml_command = [\n",
        "            \"llama-quantize\",\n",
        "            \"--imatrix\", imatrix_path,\n",
        "            fp16_path,\n",
        "            quantized_gguf_path,\n",
        "            imatrix_q_method\n",
        "        ]\n",
        "    else:\n",
        "        quantize_ggml_command = [\n",
        "            \"llama-quantize\",\n",
        "            fp16_path,\n",
        "            quantized_gguf_path,\n",
        "            q_method\n",
        "        ]\n",
        "    quantize_ggml_command_str = \" \".join(quantize_ggml_command)\n",
        "    commands_list.append(quantize_ggml_command_str)\n",
        "    return commands_list\n",
        "\n",
        "\n",
        "command_list = process_model_command(\n",
        "      model_dir_path=BASE_MODEL_PATH,\n",
        "      model_name=GGUF_NAME,\n",
        "      output_dir=OUTPUT_DIR_PATH,\n",
        "      q_method=\"Q8_0\",\n",
        "      use_imatrix=False,\n",
        "      imatrix_q_method=\"Q4_0\",\n",
        "      train_data_file=None\n",
        ")\n",
        "command = ' '.join(command_list)\n",
        "!{command}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# command = [\n",
        "#     \"python\", \"convert_hf_to_gguf.py\",\n",
        "#     BASE_MODEL_PATH\n",
        "# ]\n",
        "\n",
        "# # [\"Q2_K\", \"Q3_K_S\", \"Q3_K_M\", \"Q3_K_L\", \"Q4_0\", \"Q4_K_S\", \"Q4_K_M\", \"Q5_0\", \"Q5_K_S\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
        "# #@markdown ### Quantization Type\n",
        "# Q8_0 = True # @param {type:\"boolean\"}\n",
        "\n",
        "# q8_0_path = os.path.join(OUTPUT_DIR_PATH, f\"{OUTPUT_GGUF_NAME}-Q8_0.gguf\")\n",
        "# q_command = command + [\n",
        "#     f\"--outfile {q8_0_path}\",\n",
        "#     f\"--outtype q8_0\"\n",
        "# ]\n",
        "# q_command = ' '.join(q_command)\n",
        "# !{q_command}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13R7KrgpXerL",
        "outputId": "813f6d6e-6d63-47aa-b43c-5ade693a1e84"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
            "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}] [--bigendian]\n",
            "                             [--use-temp-file] [--no-lazy] [--model-name MODEL_NAME] [--verbose]\n",
            "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
            "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
            "                             [--no-tensor-first-split] [--metadata METADATA]\n",
            "                             [--print-supported-models]\n",
            "                             [model]\n",
            "convert_hf_to_gguf.py: error: unrecognized arguments: llama-quantize /content/drive/MyDrive/finetuning-notebooks/flux/base_models/GGUF/Flux.1-dev-fp16.gguf /content/drive/MyDrive/finetuning-notebooks/flux/base_models/GGUF/flux.1-dev-q8_0.gguf Q8_0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}