{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.0\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown To train Hunyuan Video lora 24GB VRAM is recommended.\n",
        "#@markdown <br>You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/tdrussell/diffusion-pipe\n",
        "!git clone --recurse-submodules https://github.com/tdrussell/diffusion-pipe\n",
        "%cd diffusion-pipe\n",
        "\n",
        "!pip install deepspeed\n",
        "!pip install datasets\n",
        "!pip install torch-optimi\n",
        "!pip install bitsandbytes\n",
        "!pip install av\n",
        "!pip install loguru\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training dataset.\n",
        "\n",
        "#@markdown The dataset should have following structure.\n",
        "\n",
        "#@markdown Each video file should have a corresponding text file (`.txt`) with the same name. <br>\n",
        "#@markdown **Each video must have a precious number of frames, as much as you will define later, and 24 frames per second.**\n",
        "\n",
        "#@markdown The text file typically contains prompts associated with the video.\n",
        "\n",
        "\n",
        "#@markdown ### Example Dataset Structure:\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown â”œâ”€â”€ a (1).mp4         # Video file\n",
        "#@markdown â”œâ”€â”€ a (1).txt         # Corresponding prompt for a (1).mp4\n",
        "#@markdown â”œâ”€â”€ a (2).mp4         # Another video file\n",
        "#@markdown â”œâ”€â”€ a (2).txt         # Corresponding prompt for a (2).mp4\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown This cell will download base models. If you don't already have the base model files in your google drive, run this.\n",
        "\n",
        "#@markdown You need Huggingface token (Read permission) to run this.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kqKOGVkupS_R2FQ_049xLw.png))\n",
        "\n",
        "\n",
        "#@markdown models will be downloaded from\n",
        "#@markdown - hunyuan: https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main\n",
        "#@markdown - clip: https://huggingface.co/openai/clip-vit-large-patch14\n",
        "#@markdown - llm: https://huggingface.co/Kijai/llava-llama-3-8b-text-encoder-tokenizer\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "# Set params\n",
        "BASE_MODELS_DIR_PATH = \"/content/drive/MyDrive/finetunings/hunyuan/base_models\" # @param {type:\"string\"}\n",
        "HUNYUAN_VIDEO = \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\" #@param [\"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\", \"hunyuan_video_720_cfgdistill_bf16.safetensors\", \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\"]\n",
        "VAE = \"hunyuan_video_vae_bf16.safetensors\" #@param [\"hunyuan_video_vae_bf16.safetensors\", \"hunyuan_video_vae_fp32.safetensors\"]\n",
        "CLIP = \"openai/clip-vit-large-patch14\" #@param [\"openai/clip-vit-large-patch14\"]\n",
        "LLM = \"Kijai/llava-llama-3-8b-text-encoder-tokenizer\" #@param [\"Kijai/llava-llama-3-8b-text-encoder-tokenizer\"]\n",
        "\n",
        "# Initialize dir paths\n",
        "HUNYUAN_MODELS_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "os.makedirs(HUNYUAN_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "CLIP_MODEL_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"clip\")\n",
        "os.makedirs(CLIP_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "LLM_MODEL_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"llm\")\n",
        "os.makedirs(LLM_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "HUNYUAN_VIDEO_URL = {\n",
        "    \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\",\n",
        "    \"hunyuan_video_720_cfgdistill_bf16.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_720_cfgdistill_bf16.safetensors\",\n",
        "    \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\",\n",
        "}\n",
        "\n",
        "HUNYUAN_VIDEO_VAE_URL = {\n",
        "    \"hunyuan_video_vae_bf16.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_vae_bf16.safetensors\",\n",
        "    \"hunyuan_video_vae_fp32.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_vae_fp32.safetensors\",\n",
        "}\n",
        "\n",
        "# Download models\n",
        "video_url, vae_url = HUNYUAN_VIDEO_URL[HUNYUAN_VIDEO], HUNYUAN_VIDEO_VAE_URL[VAE]\n",
        "clip_repo_id, llm_repo_id = CLIP, LLM\n",
        "\n",
        "!wget {video_url} -P {HUNYUAN_MODELS_DIR}\n",
        "!wget {vae_url} -P {HUNYUAN_MODELS_DIR}\n",
        "\n",
        "huggingface_hub.snapshot_download(\n",
        "    clip_repo_id,\n",
        "    local_dir = CLIP_MODEL_DIR,\n",
        ")\n",
        "huggingface_hub.snapshot_download(\n",
        "    llm_repo_id,\n",
        "    local_dir = LLM_MODEL_DIR,\n",
        ")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9WzQRwZij5jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import sys\n",
        "import toml\n",
        "\n",
        "#@markdown If you're intended to train Lora from checkpoint, check this.\n",
        "RESUME_FROM_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Paths Configuration\n",
        "OUTPUT_LORA_NAME = \"My-Hunyuan-Lora-V1\" # @param {type:\"string\"}\n",
        "OUTPUT_LORA_DIR_PATH = \"/content/drive/MyDrive/finetunings/outputs\"  # @param {type:\"string\"}\n",
        "BASE_MODELS_DIR_PATH = \"/content/drive/MyDrive/finetunings/hunyuan/base_models\" # @param {type:\"string\"}\n",
        "DATASET_PATH = \"/content/drive/MyDrive/finetunings/dataset/dog\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "OUTPUT_LORA_DIR_PATH = os.path.join(OUTPUT_LORA_DIR_PATH, OUTPUT_LORA_NAME)\n",
        "transformer_dir_path = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "vae_dir_path = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "clip_path = os.path.join(BASE_MODELS_DIR_PATH, \"clip\")\n",
        "llm_path = os.path.join(BASE_MODELS_DIR_PATH, \"llm\")\n",
        "\n",
        "#@markdown ## Dataset Configuration\n",
        "#@markdown - **`frame_bucket`** is the list of frame numbers in your dataset.\n",
        "#@markdown <br>For example, if your dataset contains 30, 60 frames of videos, then use : [30, 60]\n",
        "#@markdown <br>If your dataset also contains images, then use : [1, 30, 60]\n",
        "#@markdown - **`resolution`** is the resolution which **`diffusion-pipe`** will resize your dataset.\n",
        "#@markdown <br>**`diffusion-pipe`** is smart to handle resizing your dataset by 1:2 or 2:1 image etc.\n",
        "#@markdown <br>If you have less than 24GB of VRAM, just set it to 512, then increase it according to your device.\n",
        "## Frame Buckets Settings\n",
        "frame_buckets = [1]  # @param {type:\"raw\"}\n",
        "# You can use 1024 if you have 24 GB > VRAM.\n",
        "resolutions = [512]  # @param {type:\"raw\"}\n",
        "## Aspect Ratio Bucketing Settings\n",
        "enable_ar_bucket = True  # @param {type:\"boolean\"}\n",
        "min_ar = 0.5  # @param {type:\"number\"}\n",
        "max_ar = 2.0  # @param {type:\"number\"}\n",
        "num_ar_buckets = 7  # @param {type:\"integer\"}\n",
        "# Reduce as necessary\n",
        "num_repeats = 5\n",
        "\n",
        "# Write dataset.toml\n",
        "dataset_config = {\n",
        "    \"resolutions\": resolutions,\n",
        "    \"frame_buckets\": frame_buckets,\n",
        "\n",
        "    \"enable_ar_bucket\": enable_ar_bucket,\n",
        "    \"min_ar\": min_ar,\n",
        "    \"max_ar\": max_ar,\n",
        "    \"num_ar_buckets\": num_ar_buckets,\n",
        "    \"directory\": [\n",
        "        {\n",
        "            \"path\": DATASET_PATH,\n",
        "            \"num_repeats\": num_repeats,\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "os.makedirs(OUTPUT_LORA_DIR_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "dataset_config_file_path = os.path.join(DATASET_PATH, \"dataset.toml\")\n",
        "with open(dataset_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(dataset_config, toml_file)\n",
        "print(f\"dataset.toml is saved to {dataset_config_file_path}\")\n",
        "\n",
        "#@markdown ## Base Model Configuration\n",
        "BASE_HUNYUAN_MODEL_NAME = \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\" #@param [\"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\", \"hunyuan_video_720_cfgdistill_bf16.safetensors\", \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\"]\n",
        "BASE_VAE_MODEL_NAME = \"hunyuan_video_vae_bf16.safetensors\" #@param [\"hunyuan_video_vae_bf16.safetensors\", \"hunyuan_video_vae_fp32.safetensors\"]\n",
        "\n",
        "transformer_path = os.path.join(transformer_dir_path, BASE_HUNYUAN_MODEL_NAME)\n",
        "vae_path = os.path.join(vae_dir_path, BASE_VAE_MODEL_NAME)\n",
        "\n",
        "model_type = 'hunyuan-video'\n",
        "dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "transformer_dtype = 'float8'  # @param {type:\"string\"}\n",
        "timestep_sample_method = 'logit_normal'  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Training Settings\n",
        "epochs = 50  # @param {type:\"integer\"}\n",
        "micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "pipeline_stages = 1  # @param {type:\"integer\"}\n",
        "gradient_accumulation_steps = 4  # @param {type:\"integer\"}\n",
        "gradient_clipping = 1.0  # @param {type:\"number\"}\n",
        "warmup_steps = 100  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Eval Settings\n",
        "eval_every_n_epochs = 5  # @param {type:\"integer\"}\n",
        "eval_before_first_step = True  # @param {type:\"boolean\"}\n",
        "eval_micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "eval_gradient_accumulation_steps = 1  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Lora Settings\n",
        "adapter_type = 'lora'\n",
        "rank = 64  # @param {type:\"integer\"}\n",
        "adapter_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "\n",
        "# Optimizer settings\n",
        "optimizer_type = 'adamw_optimi'  # @param {type:\"string\"}\n",
        "lr = 5e-5  # @param {type:\"number\"}\n",
        "betas = [0.9, 0.99]  # @param {type:\"raw\"}\n",
        "weight_decay = 0.02  # @param {type:\"number\"}\n",
        "eps = 1e-8  # @param {type:\"number\"}\n",
        "\n",
        "#@markdown ## Misc Settings\n",
        "save_every_n_epochs = 5  # @param {type:\"integer\"}\n",
        "checkpoint_every_n_minutes = 30  # @param {type:\"integer\"}\n",
        "activation_checkpointing = True  # @param {type:\"boolean\"}\n",
        "partition_method = 'parameters'  # @param {type:\"string\"}\n",
        "save_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "caching_batch_size = 1  # @param {type:\"integer\"}\n",
        "steps_per_print = 1  # @param {type:\"integer\"}\n",
        "video_clip_mode = 'single_middle'  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Write config.toml\n",
        "train_config = {\n",
        "    \"output_dir\": OUTPUT_LORA_DIR_PATH,\n",
        "    \"dataset\": dataset_config_file_path,\n",
        "\n",
        "    # Training Settings\n",
        "    \"epochs\": epochs,\n",
        "    \"micro_batch_size_per_gpu\": micro_batch_size_per_gpu,\n",
        "    \"pipeline_stages\": pipeline_stages,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"gradient_clipping\": gradient_clipping,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "\n",
        "    # Eval Settings\n",
        "    \"eval_every_n_epochs\": eval_every_n_epochs,\n",
        "    \"eval_before_first_step\": eval_before_first_step,\n",
        "    \"eval_micro_batch_size_per_gpu\": eval_micro_batch_size_per_gpu,\n",
        "    \"eval_gradient_accumulation_steps\": eval_gradient_accumulation_steps,\n",
        "\n",
        "    # Misc Settings\n",
        "    \"save_every_n_epochs\": save_every_n_epochs,\n",
        "    \"checkpoint_every_n_minutes\": checkpoint_every_n_minutes,\n",
        "    \"activation_checkpointing\": activation_checkpointing,\n",
        "    \"partition_method\": partition_method,\n",
        "    \"save_dtype\": save_dtype,\n",
        "    \"caching_batch_size\": caching_batch_size,\n",
        "    \"steps_per_print\": steps_per_print,\n",
        "    \"video_clip_mode\": video_clip_mode,\n",
        "\n",
        "    \"model\": {\n",
        "        \"type\": model_type,\n",
        "        \"transformer_path\": transformer_path,\n",
        "        \"vae_path\": vae_path,\n",
        "        \"llm_path\": llm_path,\n",
        "        \"clip_path\": clip_path,\n",
        "        \"dtype\": dtype,\n",
        "        \"transformer_dtype\": transformer_dtype,\n",
        "        \"timestep_sample_method\": timestep_sample_method,\n",
        "    },\n",
        "\n",
        "    \"adapter\": {\n",
        "        \"type\": \"lora\",\n",
        "        \"rank\": rank,\n",
        "        \"dtype\": adapter_dtype,\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": optimizer_type,\n",
        "        \"lr\": lr,\n",
        "        \"betas\": betas,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"eps\": eps,\n",
        "    },\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "train_config_file_path = os.path.join(DATASET_PATH, \"config.toml\")\n",
        "with open(train_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(train_config, toml_file)\n",
        "print(f\"config.toml is saved to {train_config_file_path}\")\n",
        "\n",
        "\n",
        "## Train\n",
        "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
        "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path} --resume_from_checkpoint\n",
        "else:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path}\n"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}